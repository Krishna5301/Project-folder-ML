{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0349fde",
   "metadata": {},
   "source": [
    "File processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccc34f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UNSW-NB15_1.csv...\n",
      "Loading UNSW-NB15_2.csv...\n",
      "Loading UNSW-NB15_3.csv...\n",
      "Loading UNSW-NB15_4.csv...\n",
      "Combined dataset shape: (2540047, 49)\n",
      "Attack category distribution:\n",
      "attack_cat\n",
      "Generic             215481\n",
      "Exploits             44525\n",
      " Fuzzers             19195\n",
      "DoS                  16353\n",
      " Reconnaissance      12228\n",
      " Fuzzers              5051\n",
      "Analysis              2677\n",
      "Backdoor              1795\n",
      "Reconnaissance        1759\n",
      " Shellcode            1288\n",
      "Backdoors              534\n",
      "Shellcode              223\n",
      "Worms                  174\n",
      "Name: count, dtype: int64\n",
      "Balanced dataset shape: (100000, 49)\n",
      "Sampled attack category distribution:\n",
      "attack_cat\n",
      "Generic             11411\n",
      "Exploits             8024\n",
      " Fuzzers             7517\n",
      "DoS                  7464\n",
      " Reconnaissance      7400\n",
      " Fuzzers             5160\n",
      "Analysis             2726\n",
      "Backdoor             1833\n",
      "Reconnaissance       1790\n",
      " Shellcode           1310\n",
      "Backdoors             549\n",
      "Shellcode             228\n",
      "Worms                 177\n",
      "Name: count, dtype: int64\n",
      "Preprocessed dataset saved as 'unsw_nb15_preprocessed.csv'\n",
      "Final dataset columns: ['proto', 'state', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'service', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat', 'Label']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "column_headers = [\n",
    "    'srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes', 'dbytes',\n",
    "    'sttl', 'dttl', 'sloss', 'dloss', 'service', 'Sload', 'Dload', 'Spkts', 'Dpkts',\n",
    "    'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len',\n",
    "    'Sjit', 'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat',\n",
    "    'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd',\n",
    "    'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat', 'Label'\n",
    "]\n",
    "\n",
    "def merge_csv_files(file_list, headers):\n",
    "    data_frames = []\n",
    "    for path in file_list:\n",
    "        print(f\"Loading {path}...\")\n",
    "        data = pd.read_csv(path, names=headers, low_memory=False)\n",
    "        data_frames.append(data)\n",
    "    merged_data = pd.concat(data_frames, ignore_index=True)\n",
    "    print(f\"Combined dataset shape: {merged_data.shape}\")\n",
    "    return merged_data\n",
    "\n",
    "def create_balanced_sample(data, sample_size=100000):\n",
    "    if 'Label' not in data.columns or 'attack_cat' not in data.columns:\n",
    "        raise ValueError(\"Required columns missing\")\n",
    "\n",
    "    print(f\"Attack category distribution:\\n{data['attack_cat'].value_counts()}\")\n",
    "    \n",
    "    categories = data['attack_cat'].unique()\n",
    "    samples_per_category = sample_size // len(categories)\n",
    "    \n",
    "    balanced_samples = []\n",
    "    for category in categories:\n",
    "        subset = data[data['attack_cat'] == category]\n",
    "        n = min(samples_per_category, len(subset))\n",
    "        sampled_subset = subset.sample(n=n, random_state=42)\n",
    "        balanced_samples.append(sampled_subset)\n",
    "    \n",
    "    balanced_data = pd.concat(balanced_samples, ignore_index=True)\n",
    "    \n",
    "    if len(balanced_data) < sample_size:\n",
    "        additional_needed = sample_size - len(balanced_data)\n",
    "        extra_samples = data.sample(n=additional_needed, random_state=42)\n",
    "        balanced_data = pd.concat([balanced_data, extra_samples], ignore_index=True)\n",
    "    \n",
    "    print(f\"Balanced dataset shape: {balanced_data.shape}\")\n",
    "    print(f\"Sampled attack category distribution:\\n{balanced_data['attack_cat'].value_counts()}\")\n",
    "    return balanced_data\n",
    "\n",
    "def prepare_data_for_modeling(data):\n",
    "    columns_to_remove = ['srcip', 'dstip', 'sport', 'dsport', 'stcpb', 'dtcpb', 'Stime', 'Ltime']\n",
    "    data = data.drop(columns=columns_to_remove, errors='ignore')\n",
    "    \n",
    "    numeric_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for feature in numeric_features:\n",
    "        data[feature] = data[feature].fillna(data[feature].median())\n",
    "    \n",
    "    categorical_features = data.select_dtypes(include=['object']).columns\n",
    "    for feature in categorical_features:\n",
    "        data[feature] = data[feature].fillna(data[feature].mode()[0])\n",
    "        data[feature] = data[feature].astype(str)\n",
    "    \n",
    "    encoders = {}\n",
    "    for feature in categorical_features:\n",
    "        encoder = LabelEncoder()\n",
    "        data[feature] = encoder.fit_transform(data[feature])\n",
    "        encoders[feature] = encoder\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    numeric_features = numeric_features.drop('Label', errors='ignore')\n",
    "    data[numeric_features] = scaler.fit_transform(data[numeric_features])\n",
    "    \n",
    "    if data.isnull().sum().sum() > 0:\n",
    "        print(\"Warning: NaN values detected after preprocessing\")\n",
    "    if np.isinf(data[numeric_features]).sum().sum() > 0:\n",
    "        print(\"Warning: Infinite values detected after preprocessing\")\n",
    "    \n",
    "    return data, encoders, scaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_files = [\n",
    "        'UNSW-NB15_1.csv',\n",
    "        'UNSW-NB15_2.csv',\n",
    "        'UNSW-NB15_3.csv',\n",
    "        'UNSW-NB15_4.csv'\n",
    "    ]\n",
    "    \n",
    "    full_dataset = merge_csv_files(input_files, column_headers)\n",
    "    balanced_dataset = create_balanced_sample(full_dataset)\n",
    "    processed_dataset, encoders, scaler = prepare_data_for_modeling(balanced_dataset)\n",
    "    processed_dataset.to_csv('unsw_nb15_preprocessed.csv', index=False)\n",
    "    print(\"Preprocessed dataset saved as 'unsw_nb15_preprocessed.csv'\")\n",
    "    print(f\"Final dataset columns: {processed_dataset.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054eff17",
   "metadata": {},
   "source": [
    "Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2b85ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100000, 41)\n",
      "Columns in dataset: ['proto', 'state', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'service', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat', 'Label']\n",
      "Checking for missing values:\n",
      "proto               0\n",
      "state               0\n",
      "dur                 0\n",
      "sbytes              0\n",
      "dbytes              0\n",
      "sttl                0\n",
      "dttl                0\n",
      "sloss               0\n",
      "dloss               0\n",
      "service             0\n",
      "Sload               0\n",
      "Dload               0\n",
      "Spkts               0\n",
      "Dpkts               0\n",
      "swin                0\n",
      "dwin                0\n",
      "smeansz             0\n",
      "dmeansz             0\n",
      "trans_depth         0\n",
      "res_bdy_len         0\n",
      "Sjit                0\n",
      "Djit                0\n",
      "Sintpkt             0\n",
      "Dintpkt             0\n",
      "tcprtt              0\n",
      "synack              0\n",
      "ackdat              0\n",
      "is_sm_ips_ports     0\n",
      "ct_state_ttl        0\n",
      "ct_flw_http_mthd    0\n",
      "is_ftp_login        0\n",
      "ct_ftp_cmd          0\n",
      "ct_srv_src          0\n",
      "ct_srv_dst          0\n",
      "ct_dst_ltm          0\n",
      "ct_src_ltm          0\n",
      "ct_src_dport_ltm    0\n",
      "ct_dst_sport_ltm    0\n",
      "ct_dst_src_ltm      0\n",
      "attack_cat          0\n",
      "Label               0\n",
      "dtype: int64\n",
      "\n",
      "Checking for infinite values:\n",
      "proto               0\n",
      "state               0\n",
      "dur                 0\n",
      "sbytes              0\n",
      "dbytes              0\n",
      "sttl                0\n",
      "dttl                0\n",
      "sloss               0\n",
      "dloss               0\n",
      "service             0\n",
      "Sload               0\n",
      "Dload               0\n",
      "Spkts               0\n",
      "Dpkts               0\n",
      "swin                0\n",
      "dwin                0\n",
      "smeansz             0\n",
      "dmeansz             0\n",
      "trans_depth         0\n",
      "res_bdy_len         0\n",
      "Sjit                0\n",
      "Djit                0\n",
      "Sintpkt             0\n",
      "Dintpkt             0\n",
      "tcprtt              0\n",
      "synack              0\n",
      "ackdat              0\n",
      "is_sm_ips_ports     0\n",
      "ct_state_ttl        0\n",
      "ct_flw_http_mthd    0\n",
      "is_ftp_login        0\n",
      "ct_ftp_cmd          0\n",
      "ct_srv_src          0\n",
      "ct_srv_dst          0\n",
      "ct_dst_ltm          0\n",
      "ct_src_ltm          0\n",
      "ct_src_dport_ltm    0\n",
      "ct_dst_sport_ltm    0\n",
      "ct_dst_src_ltm      0\n",
      "attack_cat          0\n",
      "Label               0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "proto                 int64\n",
      "state                 int64\n",
      "dur                 float64\n",
      "sbytes              float64\n",
      "dbytes              float64\n",
      "sttl                float64\n",
      "dttl                float64\n",
      "sloss               float64\n",
      "dloss               float64\n",
      "service               int64\n",
      "Sload               float64\n",
      "Dload               float64\n",
      "Spkts               float64\n",
      "Dpkts               float64\n",
      "swin                float64\n",
      "dwin                float64\n",
      "smeansz             float64\n",
      "dmeansz             float64\n",
      "trans_depth         float64\n",
      "res_bdy_len         float64\n",
      "Sjit                float64\n",
      "Djit                float64\n",
      "Sintpkt             float64\n",
      "Dintpkt             float64\n",
      "tcprtt              float64\n",
      "synack              float64\n",
      "ackdat              float64\n",
      "is_sm_ips_ports     float64\n",
      "ct_state_ttl        float64\n",
      "ct_flw_http_mthd    float64\n",
      "is_ftp_login        float64\n",
      "ct_ftp_cmd            int64\n",
      "ct_srv_src          float64\n",
      "ct_srv_dst          float64\n",
      "ct_dst_ltm          float64\n",
      "ct_src_ltm          float64\n",
      "ct_src_dport_ltm    float64\n",
      "ct_dst_sport_ltm    float64\n",
      "ct_dst_src_ltm      float64\n",
      "attack_cat            int64\n",
      "Label                 int64\n",
      "dtype: object\n",
      "Highly correlated features to drop: ['sloss', 'dloss', 'Spkts', 'Dpkts', 'dwin', 'dmeansz', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'ct_state_ttl', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm']\n",
      "Top 25 features by importance: ['ct_state_ttl', 'sttl', 'Dload', 'dmeansz', 'dttl', 'dbytes', 'Dpkts', 'synack', 'Sload', 'ackdat', 'sbytes', 'smeansz', 'tcprtt', 'Spkts', 'dloss', 'Dintpkt', 'Sintpkt', 'dur', 'state', 'Djit', 'sloss', 'ct_dst_ltm', 'ct_dst_sport_ltm', 'ct_srv_src', 'Sjit']\n",
      "Columns after feature selection: ['ct_state_ttl', 'sttl', 'Dload', 'dmeansz', 'dttl', 'dbytes', 'Dpkts', 'synack', 'Sload', 'ackdat', 'sbytes', 'smeansz', 'tcprtt', 'Spkts', 'dloss', 'Dintpkt', 'Sintpkt', 'dur', 'state', 'Djit', 'sloss', 'ct_dst_ltm', 'ct_dst_sport_ltm', 'ct_srv_src', 'Sjit', 'attack_cat', 'Label']\n",
      "Dropped correlated features: ['sloss', 'dloss', 'Spkts', 'Dpkts', 'dwin', 'dmeansz', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'ct_state_ttl', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm']\n",
      "Dataset shape after feature engineering: (100000, 29)\n",
      "Enhanced dataset saved as 'unsw_nb15_engineered.csv'\n",
      "Final dataset columns: ['ct_state_ttl', 'sttl', 'Dload', 'dmeansz', 'dttl', 'dbytes', 'Dpkts', 'synack', 'Sload', 'ackdat', 'sbytes', 'smeansz', 'tcprtt', 'Spkts', 'dloss', 'Dintpkt', 'Sintpkt', 'dur', 'state', 'Djit', 'sloss', 'ct_dst_ltm', 'ct_dst_sport_ltm', 'ct_srv_src', 'Sjit', 'attack_cat', 'Label', 'sbytes_dbytes_ratio', 'sload_dload_ratio']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(f\"Dataset shape: {data.shape}\")\n",
    "    print(f\"Columns in dataset: {data.columns.tolist()}\")\n",
    "    return data\n",
    "\n",
    "def validate_data(data):\n",
    "    print(\"Checking for missing values:\")\n",
    "    print(data.isnull().sum())\n",
    "    print(\"\\nChecking for infinite values:\")\n",
    "    print(np.isinf(data.select_dtypes(include=['float64', 'int64'])).sum())\n",
    "    print(\"\\nData types:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "def limit_outliers(data, numeric_columns):\n",
    "    for column in numeric_columns:\n",
    "        q1 = data[column].quantile(0.25)\n",
    "        q3 = data[column].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        min_val = q1 - 1.5 * iqr\n",
    "        max_val = q3 + 1.5 * iqr\n",
    "        data[column] = data[column].clip(lower=min_val, upper=max_val)\n",
    "    return data\n",
    "\n",
    "def reduce_features(data, target_column='Label', corr_cutoff=0.8):\n",
    "    numeric_data = data.select_dtypes(include=['float64', 'int64']).columns.drop(target_column, errors='ignore')\n",
    "    corr_matrix = data[numeric_data].corr().abs()\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    redundant_features = [col for col in upper_triangle.columns if any(upper_triangle[col] > corr_cutoff)]\n",
    "    print(f\"Highly correlated features to drop: {redundant_features}\")\n",
    "    \n",
    "    features = data.drop(columns=[target_column, 'attack_cat'])\n",
    "    labels = data[target_column]\n",
    "    model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    model.fit(features, labels)\n",
    "    \n",
    "    importance_scores = pd.Series(model.feature_importances_, index=features.columns)\n",
    "    key_features = importance_scores.nlargest(25).index.tolist()\n",
    "    print(f\"Top 25 features by importance: {key_features}\")\n",
    "    \n",
    "    selected = key_features + ['attack_cat', target_column]\n",
    "    trimmed_data = data[selected]\n",
    "    print(f\"Columns after feature selection: {trimmed_data.columns.tolist()}\")\n",
    "    return trimmed_data, redundant_features\n",
    "\n",
    "def create_additional_features(data):\n",
    "    required = ['sbytes', 'dbytes', 'Sload', 'Dload']\n",
    "    missing = [col for col in required if col not in data.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    \n",
    "    data['sbytes_dbytes_ratio'] = data['sbytes'] / (data['dbytes'] + 1e-6)\n",
    "    data['sload_dload_ratio'] = data['Sload'] / (data['Dload'] + 1e-6)\n",
    "    \n",
    "    new_features = ['sbytes_dbytes_ratio', 'sload_dload_ratio']\n",
    "    scaler = StandardScaler()\n",
    "    data[new_features] = scaler.fit_transform(data[new_features])\n",
    "    \n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'unsw_nb15_preprocessed.csv'\n",
    "    dataset = read_data(input_file)\n",
    "    \n",
    "    validate_data(dataset)\n",
    "    \n",
    "    numeric_cols = dataset.select_dtypes(include=['float64', 'int64']).columns.drop('Label', errors='ignore')\n",
    "    dataset = limit_outliers(dataset, numeric_cols)\n",
    "    \n",
    "    dataset, removed = reduce_features(dataset)\n",
    "    print(f\"Dropped correlated features: {removed}\")\n",
    "    \n",
    "    dataset = create_additional_features(dataset)\n",
    "    print(f\"Dataset shape after feature engineering: {dataset.shape}\")\n",
    "    \n",
    "    dataset.to_csv('unsw_nb15_engineered.csv', index=False)\n",
    "    print(\"Enhanced dataset saved as 'unsw_nb15_engineered.csv'\")\n",
    "    print(f\"Final dataset columns: {dataset.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d9c5a",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a48b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Column 'proto' not found in dataset.\n",
      "Warning: Timestamp column 'time' not found in dataset.\n",
      "Warning: 'tuned_models.pkl' not found. Skipping XGBoost feature importance plot.\n",
      "EDA plots generated in 'eda_plots' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_context('notebook', font_scale=1.2)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "try:\n",
    "    dataset = pd.read_csv('unsw_nb15_engineered.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'unsw_nb15_engineered.csv' not found. Please ensure the file is in the working directory.\")\n",
    "    exit(1)\n",
    "\n",
    "attack_categories = {\n",
    "    0: 'Analysis', 1: 'Backdoor', 2: 'DoS', 3: 'Exploits', 4: 'Fuzzers',\n",
    "    5: 'Generic', 6: 'Reconnaissance', 7: 'Shellcode', 8: 'Worms', 9: 'Normal',\n",
    "    10: 'Class_10', 11: 'Class_11', 12: 'Class_12'\n",
    "}\n",
    "dataset['attack_cat_label'] = dataset['attack_cat'].map(attack_categories)\n",
    "\n",
    "if not os.path.exists('eda_plots'):\n",
    "    os.makedirs('eda_plots')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='attack_cat_label', data=dataset, order=dataset['attack_cat_label'].value_counts().index)\n",
    "plt.title('Distribution of Attack Categories')\n",
    "plt.xlabel('Attack Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_plots/class_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "features_to_plot = ['dur', 'spkts', 'dpkts', 'sbytes']\n",
    "valid_features = [feature for feature in features_to_plot if feature in dataset.columns]\n",
    "\n",
    "if valid_features:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for index, column in enumerate(valid_features[:4]):\n",
    "        plt.subplot(2, 2, index + 1)\n",
    "        sns.boxplot(x='attack_cat_label', y=column, data=dataset)\n",
    "        plt.title(f'Distribution of {column} by Attack Category')\n",
    "        plt.xlabel('Attack Category')\n",
    "        plt.ylabel(column)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_plots/feature_distributions.png')\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Warning: None of the specified numerical features (dur, spkts, dpkts, sbytes) found in dataset.\")\n",
    "\n",
    "numeric_columns = dataset.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_columns = [col for col in numeric_columns if col not in ['attack_cat', 'Label']]\n",
    "\n",
    "if numeric_columns:\n",
    "    correlation_data = dataset[numeric_columns].corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_data, annot=False, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix of Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_plots/correlation_matrix.png')\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Warning: No numerical columns available for correlation matrix.\")\n",
    "\n",
    "if 'proto' in dataset.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x='proto', hue='attack_cat_label', data=dataset, order=dataset['proto'].value_counts().index[:10])\n",
    "    plt.title('Attack Categories by Network Protocol')\n",
    "    plt.xlabel('Protocol')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Attack Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_plots/attack_by_protocol.png')\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Warning: Column 'proto' not found in dataset.\")\n",
    "\n",
    "if 'time' in dataset.columns:\n",
    "    try:\n",
    "        dataset['time'] = pd.to_datetime(dataset['time'], errors='coerce')\n",
    "        dataset['week'] = dataset['time'].dt.isocalendar().week\n",
    "        weekly_attacks = dataset.groupby(['week', 'attack_cat_label']).size().unstack(fill_value=0)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for attack_type in weekly_attacks.columns:\n",
    "            plt.plot(weekly_attacks.index, weekly_attacks[attack_type], label=attack_type)\n",
    "        plt.title('Weekly Attack Frequency')\n",
    "        plt.xlabel('Week of Year')\n",
    "        plt.ylabel('Number of Attacks')\n",
    "        plt.legend(title='Attack Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('eda_plots/temporal_attack_patterns.png')\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error processing temporal data: {e}\")\n",
    "else:\n",
    "    print(\"Warning: Timestamp column 'time' not found in dataset.\")\n",
    "\n",
    "try:\n",
    "    with open('tuned_models.pkl', 'rb') as file:\n",
    "        models = pickle.load(file)\n",
    "\n",
    "    model_features = dataset.drop(columns=['attack_cat', 'Label', 'attack_cat_label']).columns\n",
    "    xgb = models.get('XGBoost')\n",
    "\n",
    "    if xgb:\n",
    "        importance_scores = xgb.feature_importances_\n",
    "        top_features = np.argsort(importance_scores)[-10:]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(top_features)), importance_scores[top_features], color='teal', align='center')\n",
    "        plt.yticks(range(len(top_features)), [model_features[i] for i in top_features])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('XGBoost Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('eda_plots/feature_importance_xgboost.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Warning: XGBoost model not found in tuned_models.pkl.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'tuned_models.pkl' not found. Skipping XGBoost feature importance plot.\")\n",
    "\n",
    "print(\"EDA plots generated in 'eda_plots' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44854d4a",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831f3b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100000, 29)\n",
      "Columns: ['ct_state_ttl', 'sttl', 'Dload', 'dmeansz', 'dttl', 'dbytes', 'Dpkts', 'synack', 'Sload', 'ackdat', 'sbytes', 'smeansz', 'tcprtt', 'Spkts', 'dloss', 'Dintpkt', 'Sintpkt', 'dur', 'state', 'Djit', 'sloss', 'ct_dst_ltm', 'ct_dst_sport_ltm', 'ct_srv_src', 'Sjit', 'attack_cat', 'Label', 'sbytes_dbytes_ratio', 'sload_dload_ratio']\n",
      "Target distribution:\n",
      "attack_cat\n",
      "9     55822\n",
      "8      8024\n",
      "1      7517\n",
      "7      7464\n",
      "2      7400\n",
      "0      5160\n",
      "4      2726\n",
      "5      1833\n",
      "10     1790\n",
      "3      1310\n",
      "6       549\n",
      "11      228\n",
      "12      177\n",
      "Name: count, dtype: int64\n",
      "Train shape: (80000, 27), Test shape: (20000, 27)\n",
      "Classes: ['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic', 'Reconnaissance', 'Shellcode', 'Worms', 'Normal', 'Class_10', 'Class_11', 'Class_12']\n",
      "\n",
      "Random Forest tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best for RandomForestClassifier: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}, Score: 0.7864\n",
      "\n",
      "Random Forest Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.60      0.48      0.53      1032\n",
      "      Backdoor       0.67      0.69      0.68      1503\n",
      "           DoS       0.76      0.67      0.71      1480\n",
      "      Exploits       0.56      0.69      0.62       262\n",
      "       Fuzzers       0.30      0.23      0.26       545\n",
      "       Generic       0.10      0.34      0.15       367\n",
      "Reconnaissance       0.17      0.73      0.27       110\n",
      "     Shellcode       0.42      0.36      0.39      1493\n",
      "         Worms       0.73      0.52      0.60      1605\n",
      "        Normal       1.00      0.98      0.99     11164\n",
      "      Class_10       0.39      0.40      0.39       358\n",
      "      Class_11       0.26      0.13      0.17        46\n",
      "      Class_12       0.73      0.94      0.82        35\n",
      "\n",
      "      accuracy                           0.78     20000\n",
      "     macro avg       0.51      0.55      0.51     20000\n",
      "  weighted avg       0.81      0.78      0.79     20000\n",
      "\n",
      "\n",
      "Logistic Regression tuning...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best for LogisticRegression: {'C': 10, 'solver': 'lbfgs'}, Score: 0.6808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.45      0.29      0.35      1032\n",
      "      Backdoor       0.53      0.31      0.39      1503\n",
      "           DoS       0.50      0.16      0.24      1480\n",
      "      Exploits       0.18      0.37      0.24       262\n",
      "       Fuzzers       0.22      0.27      0.24       545\n",
      "       Generic       0.11      0.34      0.16       367\n",
      "Reconnaissance       0.10      0.73      0.17       110\n",
      "     Shellcode       0.37      0.08      0.13      1493\n",
      "         Worms       0.70      0.35      0.47      1605\n",
      "        Normal       0.99      0.96      0.98     11164\n",
      "      Class_10       0.14      0.61      0.23       358\n",
      "      Class_11       0.02      0.35      0.05        46\n",
      "      Class_12       0.05      0.80      0.09        35\n",
      "\n",
      "      accuracy                           0.66     20000\n",
      "     macro avg       0.33      0.43      0.29     20000\n",
      "  weighted avg       0.75      0.66      0.68     20000\n",
      "\n",
      "\n",
      "SVM tuning...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best for SVC: {'C': 1, 'kernel': 'linear'}, Score: 0.7211\n",
      "\n",
      "SVM Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.34      0.18      0.24      1032\n",
      "      Backdoor       0.45      0.55      0.49      1503\n",
      "           DoS       0.43      0.79      0.56      1480\n",
      "      Exploits       0.62      0.20      0.30       262\n",
      "       Fuzzers       0.56      0.05      0.08       545\n",
      "       Generic       0.00      0.00      0.00       367\n",
      "Reconnaissance       0.00      0.00      0.00       110\n",
      "     Shellcode       0.33      0.58      0.42      1493\n",
      "         Worms       0.63      0.46      0.53      1605\n",
      "        Normal       1.00      0.97      0.98     11164\n",
      "      Class_10       0.00      0.00      0.00       358\n",
      "      Class_11       0.00      0.00      0.00        46\n",
      "      Class_12       0.00      0.00      0.00        35\n",
      "\n",
      "      accuracy                           0.74     20000\n",
      "     macro avg       0.33      0.29      0.28     20000\n",
      "  weighted avg       0.74      0.74      0.72     20000\n",
      "\n",
      "\n",
      "KNN tuning...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best for KNeighborsClassifier: {'n_neighbors': 7, 'weights': 'distance'}, Score: 0.7807\n",
      "\n",
      "KNN Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.51      0.63      0.57      1032\n",
      "      Backdoor       0.63      0.64      0.64      1503\n",
      "           DoS       0.64      0.71      0.67      1480\n",
      "      Exploits       0.57      0.52      0.55       262\n",
      "       Fuzzers       0.34      0.24      0.28       545\n",
      "       Generic       0.28      0.13      0.18       367\n",
      "Reconnaissance       0.12      0.05      0.07       110\n",
      "     Shellcode       0.42      0.55      0.48      1493\n",
      "         Worms       0.61      0.55      0.58      1605\n",
      "        Normal       1.00      0.98      0.99     11164\n",
      "      Class_10       0.36      0.22      0.28       358\n",
      "      Class_11       0.20      0.04      0.07        46\n",
      "      Class_12       0.68      0.74      0.71        35\n",
      "\n",
      "      accuracy                           0.79     20000\n",
      "     macro avg       0.49      0.46      0.47     20000\n",
      "  weighted avg       0.79      0.79      0.79     20000\n",
      "\n",
      "\n",
      "XGBoost tuning...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best for XGBClassifier: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}, Score: 0.8067\n",
      "\n",
      "XGBoost Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.53      0.55      0.54      1032\n",
      "      Backdoor       0.65      0.74      0.69      1503\n",
      "           DoS       0.75      0.79      0.77      1480\n",
      "      Exploits       0.63      0.67      0.65       262\n",
      "       Fuzzers       0.73      0.21      0.32       545\n",
      "       Generic       0.75      0.13      0.22       367\n",
      "Reconnaissance       0.33      0.03      0.05       110\n",
      "     Shellcode       0.41      0.79      0.54      1493\n",
      "         Worms       0.73      0.52      0.61      1605\n",
      "        Normal       1.00      0.98      0.99     11164\n",
      "      Class_10       0.67      0.18      0.28       358\n",
      "      Class_11       0.60      0.07      0.12        46\n",
      "      Class_12       0.76      0.91      0.83        35\n",
      "\n",
      "      accuracy                           0.81     20000\n",
      "     macro avg       0.66      0.51      0.51     20000\n",
      "  weighted avg       0.84      0.81      0.81     20000\n",
      "\n",
      "\n",
      "Performance Summary:\n",
      "                 Model  Accuracy  Precision (Macro)  Recall (Macro)  \\\n",
      "0        Random Forest   0.77760           0.514314        0.550270   \n",
      "1  Logistic Regression   0.65795           0.334836        0.432811   \n",
      "2                  SVM   0.73745           0.333965        0.291252   \n",
      "3                  KNN   0.78885           0.489310        0.462776   \n",
      "4              XGBoost   0.81455           0.657155        0.505183   \n",
      "\n",
      "   F1-Score (Macro)  Precision (Weighted)  Recall (Weighted)  \\\n",
      "0          0.507905              0.812588            0.77760   \n",
      "1          0.287877              0.752234            0.65795   \n",
      "2          0.277821              0.736684            0.73745   \n",
      "3          0.465528              0.787489            0.78885   \n",
      "4          0.508973              0.836911            0.81455   \n",
      "\n",
      "   F1-Score (Weighted)  CV F1-Weighted Mean  Training Time (s)  \n",
      "0             0.790447             0.786410         120.228310  \n",
      "1             0.677225             0.680755         347.599407  \n",
      "2             0.720451             0.721053        3515.793254  \n",
      "3             0.785628             0.780654          33.151440  \n",
      "4             0.807887             0.806678         110.016263  \n",
      "\n",
      "Best Model:\n",
      "Model: XGBoost\n",
      "Weighted F1: 0.8079\n",
      "CV Mean: 0.8067, Time: 110.02s\n",
      "\n",
      "Performance saved as 'model_performance_multiclass.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    print(f\"Dataset shape: {data.shape}\")\n",
    "    print(f\"Columns: {data.columns.tolist()}\")\n",
    "    print(f\"Target distribution:\\n{data['attack_cat'].value_counts()}\")\n",
    "\n",
    "    if len(data['attack_cat'].unique()) < 2:\n",
    "        raise ValueError(\"Insufficient classes for classification.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def split_data(data, target='attack_cat'):\n",
    "    features = data.drop(columns=[target, 'Label'])\n",
    "    labels = data[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    label_names = {\n",
    "        0: 'Analysis', 1: 'Backdoor', 2: 'DoS', 3: 'Exploits', 4: 'Fuzzers',\n",
    "        5: 'Generic', 6: 'Reconnaissance', 7: 'Shellcode', 8: 'Worms', 9: 'Normal',\n",
    "        10: 'Class_10', 11: 'Class_11', 12: 'Class_12'\n",
    "    }\n",
    "    class_names = [label_names.get(i, f'Class_{i}') for i in np.unique(labels)]\n",
    "\n",
    "    print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, features.columns, class_names\n",
    "\n",
    "def optimize_model(model, params, X, y):\n",
    "    grid = GridSearchCV(model, params, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
    "    grid.fit(X, y)\n",
    "    print(f\"Best for {model.__class__.__name__}: {grid.best_params_}, Score: {grid.best_score_:.4f}\")\n",
    "    return grid.best_estimator_\n",
    "\n",
    "def show_confusion(y_true, y_pred, title, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(f'{title} Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'confusion_matrix_{title.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "def show_roc(models, X, y, labels):\n",
    "    binarizer = LabelBinarizer()\n",
    "    y_bin = binarizer.fit_transform(y)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = sns.color_palette(\"husl\", len(models))\n",
    "\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probs = model.predict_proba(X)\n",
    "            fpr, tpr, _ = roc_curve(y_bin.ravel(), probs.ravel())\n",
    "            auc = roc_auc_score(y_bin, probs, multi_class='ovr', average='macro')\n",
    "            plt.plot(fpr, tpr, color=colors[i], label=f'{name} (AUC = {auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Macro-Average ROC Curves')\n",
    "    plt.legend(loc='best', bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('macro_roc_curves.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def show_feature_importance(model, name, features):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        values = model.feature_importances_\n",
    "        top = np.argsort(values)[-10:]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(top)), values[top], align='center')\n",
    "        plt.yticks(range(len(top)), [features[i] for i in top])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'{name} Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'feature_importance_{name.lower().replace(\" \", \"_\")}.png')\n",
    "        plt.close()\n",
    "\n",
    "def compare_models(df):\n",
    "    metrics = ['Accuracy', 'Precision (Weighted)', 'Recall (Weighted)', 'F1-Score (Weighted)']\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for metric in metrics:\n",
    "        plt.plot(df['Model'], df[metric], marker='o', label=metric)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metric_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "def run_models(X_train, X_test, y_train, y_test, features, labels):\n",
    "    weights = {cls: len(y_train)/(len(np.unique(y_train)) * sum(y_train == cls)) for cls in np.unique(y_train)}\n",
    "\n",
    "    candidates = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1, class_weight=weights),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, multi_class='multinomial', max_iter=1000, class_weight=weights),\n",
    "        'SVM': SVC(random_state=42, probability=True),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'Random Forest': {'n_estimators': [100, 200], 'max_depth': [10, 20, None], 'min_samples_split': [2, 5]},\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10], 'solver': ['lbfgs', 'saga']},\n",
    "        'SVM': {'C': [0.1, 1], 'kernel': ['rbf', 'linear']},\n",
    "        'KNN': {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']},\n",
    "        'XGBoost': {'n_estimators': [100, 200], 'max_depth': [3, 6], 'learning_rate': [0.01, 0.1]}\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    final_models = {}\n",
    "\n",
    "    for name, model in candidates.items():\n",
    "        print(f\"\\n{name} tuning...\")\n",
    "        start = time.time()\n",
    "        tuned = optimize_model(model, params[name], X_train, y_train)\n",
    "        tuned.fit(X_train, y_train)\n",
    "        pred = tuned.predict(X_test)\n",
    "\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy_score(y_test, pred),\n",
    "            'Precision (Macro)': precision_score(y_test, pred, average='macro', zero_division=0),\n",
    "            'Recall (Macro)': recall_score(y_test, pred, average='macro', zero_division=0),\n",
    "            'F1-Score (Macro)': f1_score(y_test, pred, average='macro', zero_division=0),\n",
    "            'Precision (Weighted)': precision_score(y_test, pred, average='weighted', zero_division=0),\n",
    "            'Recall (Weighted)': recall_score(y_test, pred, average='weighted', zero_division=0),\n",
    "            'F1-Score (Weighted)': f1_score(y_test, pred, average='weighted', zero_division=0),\n",
    "            'CV F1-Weighted Mean': cross_val_score(tuned, X_train, y_train, cv=5, scoring='f1_weighted', n_jobs=-1).mean(),\n",
    "            'Training Time (s)': time.time() - start\n",
    "        })\n",
    "\n",
    "        final_models[name] = tuned\n",
    "        show_confusion(y_test, pred, name, labels)\n",
    "        show_feature_importance(tuned, name, features)\n",
    "        print(f\"\\n{name} Report:\\n{classification_report(y_test, pred, target_names=labels, zero_division=0)}\")\n",
    "\n",
    "    show_roc(final_models, X_test, y_test, labels)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(df_results)\n",
    "    compare_models(df_results)\n",
    "\n",
    "    preds = {name: model.predict(X_test) for name, model in final_models.items()}\n",
    "    pd.DataFrame(preds).to_csv('model_predictions.csv', index=False)\n",
    "    pd.Series(y_test, name='actual').to_csv('y_test.csv', index=False)\n",
    "    with open('tuned_models.pkl', 'wb') as f:\n",
    "        pickle.dump(final_models, f)\n",
    "\n",
    "    return df_results, final_models\n",
    "\n",
    "def choose_best(df):\n",
    "    best = df.loc[df['F1-Score (Weighted)'].idxmax()]\n",
    "    print(\"\\nBest Model:\")\n",
    "    print(f\"Model: {best['Model']}\")\n",
    "    print(f\"Weighted F1: {best['F1-Score (Weighted)']:.4f}\")\n",
    "    print(f\"CV Mean: {best['CV F1-Weighted Mean']:.4f}, Time: {best['Training Time (s)']:.2f}s\")\n",
    "    return best\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data('unsw_nb15_engineered.csv')\n",
    "    X_train, X_test, y_train, y_test, feature_names, class_labels = split_data(df)\n",
    "    results, models = run_models(X_train, X_test, y_train, y_test, feature_names, class_labels)\n",
    "    best = choose_best(results)\n",
    "    results.to_csv('model_performance_multiclass.csv', index=False)\n",
    "    print(\"\\nPerformance saved as 'model_performance_multiclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c65df",
   "metadata": {},
   "source": [
    "Model performance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc4e6ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in performance file: ['Model', 'Accuracy', 'Precision (Macro)', 'Recall (Macro)', 'F1-Score (Macro)', 'Precision (Weighted)', 'Recall (Weighted)', 'F1-Score (Weighted)', 'CV F1-Weighted Mean', 'Training Time (s)']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load model performance data\n",
    "performance = pd.read_csv('model_performance_multiclass.csv')\n",
    "\n",
    "# Check available columns\n",
    "print(\"Available columns in performance file:\", performance.columns.tolist())\n",
    "\n",
    "# Plot bar chart for weighted F1-Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='F1-Score (Weighted)', data=performance)\n",
    "plt.title('Weighted F1-Score Comparison')\n",
    "plt.ylabel('Weighted F1-Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visual_f1_score_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot training time\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Training Time (s)', data=performance)\n",
    "plt.title('Training Time by Model')\n",
    "plt.ylabel('Training Time (s)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visual_training_time_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot CV F1-Weighted Mean if available\n",
    "if 'CV F1-Weighted Mean' in performance.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Model', y='CV F1-Weighted Mean', data=performance)\n",
    "    plt.title('Cross-Validation F1-Weighted Mean')\n",
    "    plt.ylabel('CV F1-Weighted Mean')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visual_cv_f1_weighted_mean.png')\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"CV F1-Weighted Mean not found in CSV.\")\n",
    "\n",
    "# Optional violin plot to show variability (only if Std column exists)\n",
    "if 'CV F1-Weighted Std' in performance.columns:\n",
    "    cv_distributions = {}\n",
    "    for model_name in performance['Model']:\n",
    "        mean_score = performance.loc[performance['Model'] == model_name, 'CV F1-Weighted Mean'].values[0]\n",
    "        std_dev = performance.loc[performance['Model'] == model_name, 'CV F1-Weighted Std'].values[0]\n",
    "        # Simulate distribution\n",
    "        cv_distributions[model_name] = np.random.normal(mean_score, std_dev, 1000)\n",
    "\n",
    "    dist_df = pd.DataFrame(cv_distributions)\n",
    "    dist_df = dist_df.melt(var_name='Model', value_name='CV F1 Score')\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.violinplot(x='Model', y='CV F1 Score', data=dist_df)\n",
    "    plt.title('Simulated CV F1-Score Distribution')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visual_violin_cv_f1_score_distribution.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
